{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b055750a",
   "metadata": {},
   "source": [
    "## Fine-Tune a Generative AI Model for Dialogue Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4afdae",
   "metadata": {},
   "source": [
    "In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8e606",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "### 1 - Load Required Dependencies, Dataset and LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2589b99",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "#### 1.1 - Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdceb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df9432",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "#### 1.2 - Load Dataset and LLM\n",
    "\n",
    "We are going to use the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2609bd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fa2d9",
   "metadata": {},
   "source": [
    "Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5977fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with appropriate device mapping\n",
    "# If CUDA is not available, force CPU usage\n",
    "if torch.cuda.is_available():\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", dtype=torch.bfloat16)\n",
    "else:\n",
    "    # Use float32 on CPU since bfloat16 may not be supported\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"cpu\", dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e49a516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 247577856 || all params: 247577856 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "# determine the number of trainable parameters in the model\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e529855",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "#### 1.3 - Test the Model with Zero Shot Inferencing\n",
    "\n",
    "Test the model with the zero shot inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be582036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dialogue: #Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n",
      "#Person2#: What was the problem that time?\n",
      "#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n",
      "#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n",
      "#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n",
      "#Person2#: I'm not so sure about that.\n",
      "#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n",
      "\n",
      "Summary: #Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Generated Summary: The two of them are trying to figure out how to react to Jason's reaction.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dialogue: #Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n",
      "#Person2#: What was the problem that time?\n",
      "#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n",
      "#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n",
      "#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n",
      "#Person2#: I'm not so sure about that.\n",
      "#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n",
      "\n",
      "Summary: #Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Generated Summary: The two of them are trying to figure out how to react to Jason's reaction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model with zero-shot learning\n",
    "index = 100\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "prompt = f\"\"\"summarize the following conversation: \n",
    "{dialogue}\n",
    "summary: \"\"\"\n",
    "\n",
    "# Use CUDA if available, otherwise use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = tokenizer.decode(\n",
    "    model.generate(**inputs, max_new_tokens=100)[0], \n",
    "    skip_special_tokens=True)\n",
    "\n",
    "dashline = \"-\" * 100\n",
    "print(dashline)\n",
    "print(f\"Dialogue: {dialogue}\\n\")\n",
    "print(f\"Summary: {summary}\\n\")\n",
    "print(dashline)\n",
    "print(f\"Generated Summary: {outputs}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03c6a5",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "### 2 - Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4556a1",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "#### 2.1 - Preprocess the Dialog-Summary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8281c895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5549b1bbee50424b9899fec32f39c958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': List(Value('int32')), 'labels': List(Value('int64'))}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a tokenization function to preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = [\n",
    "        f\"summarize the following conversation: \\n{dialogue}\\nsummary: \" for dialogue in examples[\"dialogue\"]\n",
    "    ]\n",
    "    examples[\"input_ids\"] = tokenizer(inputs, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    examples[\"labels\"] = tokenizer(\n",
    "        examples[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    return examples\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"dialogue\", \"summary\", \"topic\", \"id\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n",
    "tokenized_datasets[\"train\"].features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1229fbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c35e60eeeb40e48a66b2e633e0910a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2eb40a9710e43e3852078eddf8cac20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsample the dataset for quicker training (increased from every 50th to every 100th to save memory)\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, idx: idx % 50 == 0, with_indices=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00183129",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "#### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n",
    "\n",
    "We will utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854d914e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "# define the training arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1,  # for quicker testing; remove or increase for actual training\n",
    "    per_device_train_batch_size=1,  # Reduce batch size to save memory\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "    fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),  # Use mixed precision if available\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),  # Use bfloat16 if supported\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11bc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90645028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>47.211600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=47.21155548095703, metrics={'train_runtime': 3.5975, 'train_samples_per_second': 1.112, 'train_steps_per_second': 0.278, 'total_flos': 2739029409792.0, 'train_loss': 47.21155548095703, 'epoch': 0.016})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf9bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./finetuned-flan-t5-base\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "output_dir = \"./finetuned-flan-t5-base\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f0545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(output_dir, device_map=\"auto\", dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32)\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721b508",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "#### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed6a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\Downloads\\Coursera\\GenAI with LLM\\llm-finetune-conversation-summary-lora\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Caching is incompatible with gradient checkpointing in T5Block. Setting `past_key_values=None`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Dialogue:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Baseline Summary:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Original Summary:\n",
      "Brian     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine-tuned Summary:\n",
      "Brian's birthday is coming up.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dialogue:\n",
      "#Person1#: Yeah. Just pull on this strip. Then peel off the back.\n",
      "#Person2#: You might make a few enemies this way.\n",
      "#Person1#: If they don't think this is fun, they're not meant to be our friends.\n",
      "#Person2#: You mean your friends. I think it's cruel.\n",
      "#Person1#: Yeah. But it's fun. Look at those two ugly old ladies. . . or are they men?\n",
      "#Person2#: Hurry! Get a shot!. . . Hand it over!\n",
      "#Person1#: I knew you'd come around. . .\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Baseline Summary:\n",
      "#Person1# is about to make a prank. #Person2# thinks it's cruel at first but then joins.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Original Summary:\n",
      "'' aaa.......................\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine-tuned Summary:\n",
      "#Person1#: Okay.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Dialogue:\n",
      "#Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n",
      "#Person2#: What was the problem that time?\n",
      "#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n",
      "#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n",
      "#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n",
      "#Person2#: I'm not so sure about that.\n",
      "#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Baseline Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Original Summary:\n",
      "The\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine-tuned Summary:\n",
      "The two of them are trying to figure out how to react to Jason's reaction.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on a few test examples\n",
    "for index in [10, 50, 100]:\n",
    "    dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "    baseline_summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "    prompt = f\"\"\"summarize the following conversation:\n",
    "    {dialogue}\n",
    "    summary: \"\"\"\n",
    "\n",
    "    inputs = finetuned_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    original_model_output = model.generate(**inputs, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_summary = tokenizer.decode(original_model_output[0], skip_special_tokens=True)\n",
    "\n",
    "    finetuned_model_output = finetuned_model.generate(**inputs)\n",
    "    finetuned_summary = finetuned_tokenizer.decode(finetuned_model_output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dashline)\n",
    "    print(f\"Dialogue:\\n{dialogue}\")\n",
    "    print(dashline)\n",
    "    print(f\"Baseline Summary:\\n{baseline_summary}\")\n",
    "    print(dashline)\n",
    "    print(f\"Original Summary:\\n{original_summary}\")\n",
    "    print(dashline)\n",
    "    print(f\"Fine-tuned Summary:\\n{finetuned_summary}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f1238",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "#### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
    "\n",
    "The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "301f930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'pad_token_id': 0, 'eos_token_id': 1, 'decoder_start_token_id': 0}. If this is not desired, please set these values explicitly.\n",
      "c:\\Users\\samue\\Downloads\\Coursera\\GenAI with LLM\\llm-finetune-conversation-summary-lora\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Original Prediction</th>\n",
       "      <th>Fine-tuned Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Person1# wants to know the result of a interv...</td>\n",
       "      <td># # # # # # # # # # # # # # # # # # # # # # # ...</td>\n",
       "      <td>Interviewers ask for the results of the interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Person1# and #Person2# are appreciating lante...</td>\n",
       "      <td>People None</td>\n",
       "      <td>The Lantern Festival is taking place in Beijing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Person1# apologises to #Person2# after the qu...</td>\n",
       "      <td>Person.</td>\n",
       "      <td>Person1: Honey, I'm sorry I didn't call you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person1# asks for #Person2#'s help to print u...</td>\n",
       "      <td>Person mj</td>\n",
       "      <td>#Person1#: I need to edit my paper.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2#'s attachment exceeds the e-mail capa...</td>\n",
       "      <td>The___</td>\n",
       "      <td>#Person1#: I'm sorry, I can't send out this e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person1# and #Person2# are talking about natu...</td>\n",
       "      <td>People</td>\n",
       "      <td>The earthquake in Wenchuan in China is a natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# feels bored at home and asks Jim go ...</td>\n",
       "      <td># # # # # # # # # # # #</td>\n",
       "      <td>Jim and Mary are going to the gym. They are go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# complains to Tony that Christmas has...</td>\n",
       "      <td># # # # # # # # # # # # # # # # # # # # # # # ...</td>\n",
       "      <td>#Person1: Hi, Tony.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person2# draws #Person1#'s blood to check whi...</td>\n",
       "      <td>The The............sssssssssssssssssssssssssss</td>\n",
       "      <td>The doctor will take a blood test.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# helps #Person2# pick a gift for #Per...</td>\n",
       "      <td>It.</td>\n",
       "      <td>#Person1#: I'm looking for a nice gift for my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Reference  \\\n",
       "0  #Person1# wants to know the result of a interv...   \n",
       "1  #Person1# and #Person2# are appreciating lante...   \n",
       "2  #Person1# apologises to #Person2# after the qu...   \n",
       "3  #Person1# asks for #Person2#'s help to print u...   \n",
       "4  #Person2#'s attachment exceeds the e-mail capa...   \n",
       "5  #Person1# and #Person2# are talking about natu...   \n",
       "6  #Person1# feels bored at home and asks Jim go ...   \n",
       "7  #Person1# complains to Tony that Christmas has...   \n",
       "8  #Person2# draws #Person1#'s blood to check whi...   \n",
       "9  #Person1# helps #Person2# pick a gift for #Per...   \n",
       "\n",
       "                                 Original Prediction  \\\n",
       "0  # # # # # # # # # # # # # # # # # # # # # # # ...   \n",
       "1                                        People None   \n",
       "2                                            Person.   \n",
       "3                                          Person mj   \n",
       "4                                             The___   \n",
       "5                                             People   \n",
       "6                            # # # # # # # # # # # #   \n",
       "7  # # # # # # # # # # # # # # # # # # # # # # # ...   \n",
       "8     The The............sssssssssssssssssssssssssss   \n",
       "9                                                It.   \n",
       "\n",
       "                               Fine-tuned Prediction  \n",
       "0  Interviewers ask for the results of the interv...  \n",
       "1   The Lantern Festival is taking place in Beijing.  \n",
       "2       Person1: Honey, I'm sorry I didn't call you.  \n",
       "3                #Person1#: I need to edit my paper.  \n",
       "4  #Person1#: I'm sorry, I can't send out this e-...  \n",
       "5  The earthquake in Wenchuan in China is a natur...  \n",
       "6  Jim and Mary are going to the gym. They are go...  \n",
       "7                                #Person1: Hi, Tony.  \n",
       "8                 The doctor will take a blood test.  \n",
       "9  #Person1#: I'm looking for a nice gift for my ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 10 random samples from the test set for evaluation\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(len(dataset[\"test\"]), size=10, replace=False)\n",
    "samples = [dataset[\"test\"][i] for i in random_indices]\n",
    "# Prepare references and predictions for ROUGE evaluation\n",
    "references = [sample[\"summary\"] for sample in samples]\n",
    "original_predictions = []\n",
    "finetuned_predictions = []\n",
    "for sample in samples:\n",
    "    dialogue = sample[\"dialogue\"]\n",
    "    prompt = f\"\"\"summarize the following conversation:\n",
    "    {dialogue}\n",
    "    summary: \"\"\"\n",
    "    inputs = finetuned_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    finetuned_model_output = finetuned_model.generate(**inputs, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    finetuned_summary = finetuned_tokenizer.decode(finetuned_model_output[0], skip_special_tokens=True)\n",
    "    finetuned_predictions.append(finetuned_summary)\n",
    "\n",
    "    original_model_output = model.generate(**inputs, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_summary = tokenizer.decode(original_model_output[0], skip_special_tokens=True)\n",
    "    original_predictions.append(original_summary)\n",
    "\n",
    "# Zip the references and predictions together for evaluation and generate a dataframe\n",
    "results = list(zip(references, original_predictions, finetuned_predictions))\n",
    "df = pd.DataFrame(results, columns=[\"Reference\", \"Original Prediction\", \"Fine-tuned Prediction\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7bda1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model ROUGE Scores: {'rouge1': np.float64(0.22936643822127695), 'rouge2': np.float64(0.05641113351684296), 'rougeL': np.float64(0.1894295167198393), 'rougeLsum': np.float64(0.18803225806451612)}\n",
      "Original Model ROUGE Scores: {'rouge1': np.float64(0.015384615384615385), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.015384615384615385), 'rougeLsum': np.float64(0.015384615384615385)}\n",
      "Original Model ROUGE Scores: {'rouge1': np.float64(0.015384615384615385), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.015384615384615385), 'rougeLsum': np.float64(0.015384615384615385)}\n"
     ]
    }
   ],
   "source": [
    "# combine the above two cells to show both results together\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge.add_batch(predictions=df[\"Fine-tuned Prediction\"].tolist(), references=df[\"Reference\"].tolist())\n",
    "rouge_finetuned_scores = rouge.compute()\n",
    "print(\"Fine-tuned Model ROUGE Scores:\", rouge_finetuned_scores)\n",
    "\n",
    "rouge_original = evaluate.load(\"rouge\")\n",
    "rouge_original.add_batch(predictions=df[\"Original Prediction\"].tolist(), references=df[\"Reference\"].tolist())\n",
    "rouge_original_scores = rouge_original.compute()\n",
    "print(\"Original Model ROUGE Scores:\", rouge_original_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dea95b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in ROUGE Scores (Fine-tuned - Original): {'rouge1': np.float64(13.908818484383001), 'rouge2': 0, 'rougeL': np.float64(11.312918586789554), 'rougeLsum': np.float64(11.222096774193547)}\n"
     ]
    }
   ],
   "source": [
    "# compute the difference in ROUGE scores in percentage\n",
    "def compute_rouge_difference(rouge_finetuned, rouge_original):\n",
    "    difference = {}\n",
    "    for key in rouge_finetuned.keys():\n",
    "        if rouge_original.get(key, 0) != 0:\n",
    "            difference[key] = ((rouge_finetuned[key] - rouge_original[key]) / rouge_original[key])\n",
    "        else:\n",
    "            difference[key] = 0\n",
    "    return difference\n",
    "difference = compute_rouge_difference(rouge_finetuned_scores, rouge_original_scores)\n",
    "print(\"Difference in ROUGE Scores (Fine-tuned - Original):\", difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc5f19",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "### 3 - Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "**Parameter Efficient Fine-Tuning (PEFT)** is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results. PEFT includes **Low-Rank Adaptation (LoRA)**. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs). At inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f22548",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "#### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06eb5d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1769472 || all params: 249347328 || trainable%: 0.7096414524241463\n"
     ]
    }
   ],
   "source": [
    "# Set up a PEFT/LoRA model for fine-tuning\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"],  # Targeting query and value projection layers\n",
    "    bias=\"none\" # No bias adaptation\n",
    ")\n",
    "# Create the PEFT/LoRA model (this will modify the original model in place and return it as well)\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb6bef",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "#### 3.2 - Train PEFT Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3081c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"./peft-results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./peft-results/logs\",\n",
    "    logging_steps=1,\n",
    "    max_steps=1,  # for quicker testing; remove or increase for actual training\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "    fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),  # Use mixed precision if available\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e8a207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory before training\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d017203b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>47.403700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=47.403656005859375, metrics={'train_runtime': 1.4453, 'train_samples_per_second': 2.768, 'train_steps_per_second': 0.692, 'total_flos': 2760772681728.0, 'train_loss': 47.403656005859375, 'epoch': 0.016})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the PEFT/LoRA model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5a0026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for inference and save the PEFT/LoRA adapter locally\n",
    "lora_model.save_pretrained(\"./peft-results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5ba533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 249347328 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "# Load the PEFT/LoRA model for inference\n",
    "peft_base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "peft_model = PeftModel.from_pretrained(peft_base_model, \"./peft-results\", is_training=False, dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32)\n",
    "\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc227e4",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "#### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalute the model\n",
    "\n",
    "index = 100\n",
    "dialogue = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dc6ceb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\Downloads\\Coursera\\GenAI with LLM\\llm-finetune-conversation-summary-lora\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Dialogue:\n",
      "#Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n",
      "#Person2#: What was the problem that time?\n",
      "#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n",
      "#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n",
      "#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n",
      "#Person2#: I'm not so sure about that.\n",
      "#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Base Summary:\n",
      "#Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "PEFT Model Output:\n",
      "The two of them are trying to figure out how to react to Jason's reaction.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fine-tuned Model Output:\n",
      "The two of them are trying to figure out how to react to Jason's reaction.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Original Model Output:\n",
      "Jason\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model qualitatively (human evaluation)\n",
    "index = 100\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "base_summary = dataset[\"test\"][index][\"summary\"]\n",
    "prompt = f\"\"\"summarize the following conversation:\n",
    "{dialogue}\n",
    "summary: \"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Move models to the correct device if needed\n",
    "peft_model = peft_model.to(device)\n",
    "finetuned_model = finetuned_model.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "peft_outputs = tokenizer.decode(\n",
    "    peft_model.generate(**inputs, max_new_tokens=200)[0], \n",
    "    skip_special_tokens=True)\n",
    "\n",
    "original_outputs = tokenizer.decode(\n",
    "    model.generate(**inputs, max_new_tokens=200)[0],\n",
    "    skip_special_tokens=True)\n",
    "\n",
    "finetuned_outputs = tokenizer.decode(\n",
    "    finetuned_model.generate(**inputs, max_new_tokens=200)[0],\n",
    "    skip_special_tokens=True)\n",
    "\n",
    "print(dashline)\n",
    "print(f\"Dialogue:\\n{dialogue}\\n\")\n",
    "print(dashline)\n",
    "print(f\"Base Summary:\\n{base_summary}\\n\")\n",
    "print(dashline)\n",
    "print(f\"PEFT Model Output:\\n{peft_outputs}\\n\")\n",
    "print(dashline)\n",
    "print(f\"Fine-tuned Model Output:\\n{finetuned_outputs}\\n\")\n",
    "print(dashline)\n",
    "print(f\"Original Model Output:\\n{original_outputs}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e68be",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f54798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\Downloads\\Coursera\\GenAI with LLM\\llm-finetune-conversation-summary-lora\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Original Prediction</th>\n",
       "      <th>Fine-tuned Prediction</th>\n",
       "      <th>PEFT Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Person1# wants to know the result of a interv...</td>\n",
       "      <td>You           ...................................</td>\n",
       "      <td>Interviewers ask for the results of the interv...</td>\n",
       "      <td>Interviewers ask for the results of the interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Person1# and #Person2# are appreciating lante...</td>\n",
       "      <td>People</td>\n",
       "      <td>The Lantern Festival is taking place in Beijing.</td>\n",
       "      <td>The Lantern Festival is taking place in Beijing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Person1# apologises to #Person2# after the qu...</td>\n",
       "      <td>Person      ......</td>\n",
       "      <td>Person1: Honey, I'm sorry I didn't call you.</td>\n",
       "      <td>Person1: Honey, I'm sorry I didn't call you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person1# asks for #Person2#'s help to print u...</td>\n",
       "      <td># #</td>\n",
       "      <td>#Person1#: I need to edit my paper.</td>\n",
       "      <td>#Person1#: I need to edit my paper.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2#'s attachment exceeds the e-mail capa...</td>\n",
       "      <td># # # # # # # # # # # # # # # # # # # # # # # ...</td>\n",
       "      <td>#Person1#: I'm sorry, I can't send out this e-...</td>\n",
       "      <td>#Person1#: I'm sorry, I can't send out this e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person1# and #Person2# are talking about natu...</td>\n",
       "      <td># # # # # # # # # # # # # # # # # # # # # # # ...</td>\n",
       "      <td>The earthquake in Wenchuan in China is a natur...</td>\n",
       "      <td>The earthquake in Wenchuan in China is a natur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# feels bored at home and asks Jim go ...</td>\n",
       "      <td>Person pzzi</td>\n",
       "      <td>Jim and Mary are going to the gym. They are go...</td>\n",
       "      <td>Jim and Mary are going to the gym. They are go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# complains to Tony that Christmas has...</td>\n",
       "      <td>A A..............................................</td>\n",
       "      <td>#Person1: Hi, Tony.</td>\n",
       "      <td>The toy department at the shopping center is b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person2# draws #Person1#'s blood to check whi...</td>\n",
       "      <td>Theo</td>\n",
       "      <td>The doctor will take a blood test.</td>\n",
       "      <td>The doctor will take a blood test.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# helps #Person2# pick a gift for #Per...</td>\n",
       "      <td># # # # # # # # # # # # # # # # # # # # # # # ...</td>\n",
       "      <td>#Person1#: I'm looking for a nice gift for my ...</td>\n",
       "      <td>#Person1#: I'm looking for a nice gift for my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Reference  \\\n",
       "0  #Person1# wants to know the result of a interv...   \n",
       "1  #Person1# and #Person2# are appreciating lante...   \n",
       "2  #Person1# apologises to #Person2# after the qu...   \n",
       "3  #Person1# asks for #Person2#'s help to print u...   \n",
       "4  #Person2#'s attachment exceeds the e-mail capa...   \n",
       "5  #Person1# and #Person2# are talking about natu...   \n",
       "6  #Person1# feels bored at home and asks Jim go ...   \n",
       "7  #Person1# complains to Tony that Christmas has...   \n",
       "8  #Person2# draws #Person1#'s blood to check whi...   \n",
       "9  #Person1# helps #Person2# pick a gift for #Per...   \n",
       "\n",
       "                                 Original Prediction  \\\n",
       "0  You           ...................................   \n",
       "1                                             People   \n",
       "2                                 Person      ......   \n",
       "3                                                # #   \n",
       "4  # # # # # # # # # # # # # # # # # # # # # # # ...   \n",
       "5  # # # # # # # # # # # # # # # # # # # # # # # ...   \n",
       "6                                        Person pzzi   \n",
       "7  A A..............................................   \n",
       "8                                               Theo   \n",
       "9  # # # # # # # # # # # # # # # # # # # # # # # ...   \n",
       "\n",
       "                               Fine-tuned Prediction  \\\n",
       "0  Interviewers ask for the results of the interv...   \n",
       "1   The Lantern Festival is taking place in Beijing.   \n",
       "2       Person1: Honey, I'm sorry I didn't call you.   \n",
       "3                #Person1#: I need to edit my paper.   \n",
       "4  #Person1#: I'm sorry, I can't send out this e-...   \n",
       "5  The earthquake in Wenchuan in China is a natur...   \n",
       "6  Jim and Mary are going to the gym. They are go...   \n",
       "7                                #Person1: Hi, Tony.   \n",
       "8                 The doctor will take a blood test.   \n",
       "9  #Person1#: I'm looking for a nice gift for my ...   \n",
       "\n",
       "                                     PEFT Prediction  \n",
       "0  Interviewers ask for the results of the interv...  \n",
       "1   The Lantern Festival is taking place in Beijing.  \n",
       "2       Person1: Honey, I'm sorry I didn't call you.  \n",
       "3                #Person1#: I need to edit my paper.  \n",
       "4  #Person1#: I'm sorry, I can't send out this e-...  \n",
       "5  The earthquake in Wenchuan in China is a natur...  \n",
       "6  Jim and Mary are going to the gym. They are go...  \n",
       "7  The toy department at the shopping center is b...  \n",
       "8                 The doctor will take a blood test.  \n",
       "9  #Person1#: I'm looking for a nice gift for my ...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify below to include the peft model\n",
    "\n",
    "# Select 10 random samples from the test set for evaluation\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(len(dataset[\"test\"]), size=10, replace=False)\n",
    "samples = [dataset[\"test\"][i] for i in random_indices]\n",
    "# Prepare references and predictions for ROUGE evaluation\n",
    "references = [sample[\"summary\"] for sample in samples]\n",
    "original_predictions = []\n",
    "finetuned_predictions = []\n",
    "peft_predictions = []\n",
    "for sample in samples:\n",
    "    dialogue = sample[\"dialogue\"]\n",
    "    prompt = f\"\"\"summarize the following conversation:\n",
    "    {dialogue}\n",
    "    summary: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # PEFT model predictions\n",
    "    peft_model_output = peft_model.generate(**inputs, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_summary = tokenizer.decode(peft_model_output[0], skip_special_tokens=True)\n",
    "    peft_predictions.append(peft_summary)\n",
    "    \n",
    "    # Fine-tuned model predictions\n",
    "    finetuned_model_output = finetuned_model.generate(**inputs, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    finetuned_summary = finetuned_tokenizer.decode(finetuned_model_output[0], skip_special_tokens=True)\n",
    "    finetuned_predictions.append(finetuned_summary)\n",
    "\n",
    "    # Original model predictions\n",
    "    original_model_output = model.generate(**inputs, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_summary = tokenizer.decode(original_model_output[0], skip_special_tokens=True)\n",
    "    original_predictions.append(original_summary)\n",
    "\n",
    "# Zip the references and predictions together for evaluation and generate a dataframe\n",
    "results = list(zip(references, original_predictions, finetuned_predictions, peft_predictions))\n",
    "df = pd.DataFrame(results, columns=[\"Reference\", \"Original Prediction\", \"Fine-tuned Prediction\", \"PEFT Prediction\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "964ff5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model ROUGE Scores: {'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'rougeLsum': np.float64(0.0)}\n",
      "Fine-tuned Model ROUGE Scores: {'rouge1': np.float64(0.22936643822127695), 'rouge2': np.float64(0.05641113351684296), 'rougeL': np.float64(0.1894295167198393), 'rougeLsum': np.float64(0.18803225806451612)}\n",
      "PEFT Model ROUGE Scores: {'rouge1': np.float64(0.19810933868998387), 'rouge2': np.float64(0.05641113351684296), 'rougeL': np.float64(0.1591181220213478), 'rougeLsum': np.float64(0.15888412304541336)}\n",
      "\n",
      "Percentage differences (relative to original model):\n",
      "rouge1_finetuned_vs_original: 0.00%\n",
      "rouge1_peft_vs_original: 0.00%\n",
      "rouge2_finetuned_vs_original: 0.00%\n",
      "rouge2_peft_vs_original: 0.00%\n",
      "rougeL_finetuned_vs_original: 0.00%\n",
      "rougeL_peft_vs_original: 0.00%\n",
      "rougeLsum_finetuned_vs_original: 0.00%\n",
      "rougeLsum_peft_vs_original: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Compute ROUGE scores for across all three models using the dataframe and calculate percentage differences\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "original_rouge = rouge.compute(predictions=df[\"Original Prediction\"].tolist(), references=df[\"Reference\"].tolist())\n",
    "finetuned_rouge = rouge.compute(predictions=df[\"Fine-tuned Prediction\"].tolist(), references=df[\"Reference\"].tolist())\n",
    "peft_rouge = rouge.compute(predictions=df[\"PEFT Prediction\"].tolist(), references=df[\"Reference\"].tolist())\n",
    "\n",
    "print(\"Original Model ROUGE Scores:\", original_rouge)\n",
    "print(\"Fine-tuned Model ROUGE Scores:\", finetuned_rouge)\n",
    "print(\"PEFT Model ROUGE Scores:\", peft_rouge)\n",
    "\n",
    "# Calculate percentage differences\n",
    "def compute_percentage_difference(scores1, scores2, baseline_scores):\n",
    "    \"\"\"Compute percentage difference between scores1 and scores2 relative to baseline\"\"\"\n",
    "    difference = {}\n",
    "    for key in scores1.keys():\n",
    "        if baseline_scores.get(key, 0) != 0:\n",
    "            diff1 = ((scores1[key] - baseline_scores[key]) / baseline_scores[key]) * 100\n",
    "            diff2 = ((scores2[key] - baseline_scores[key]) / baseline_scores[key]) * 100\n",
    "            difference[f\"{key}_finetuned_vs_original\"] = diff1\n",
    "            difference[f\"{key}_peft_vs_original\"] = diff2\n",
    "        else:\n",
    "            difference[f\"{key}_finetuned_vs_original\"] = 0\n",
    "            difference[f\"{key}_peft_vs_original\"] = 0\n",
    "    return difference\n",
    "\n",
    "percentage_differences = compute_percentage_difference(finetuned_rouge, peft_rouge, original_rouge)\n",
    "print(\"\\nPercentage differences (relative to original model):\")\n",
    "for key, value in percentage_differences.items():\n",
    "    print(f\"{key}: {value:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738775ab",
   "metadata": {},
   "source": [
    "### 4 - GPU-Optimized Training Improvements\n",
    "\n",
    "This section contains optimized configurations to significantly improve model performance using your local GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d079f",
   "metadata": {},
   "source": [
    "### 4.1 - GPU Memory Optimization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8628f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TF32 enabled for faster training\n",
      "GPU Device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Total Memory: 6.00 GB\n",
      "Allocated: 1.47 GB\n",
      "Reserved: 1.49 GB\n",
      "Free: 4.51 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU Memory Analysis and Optimization Setup\n",
    "import torch\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "        print(f\"Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1024**3:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "# Enable optimizations for faster training on modern GPUs\n",
    "if torch.cuda.is_available():\n",
    "    # Enable TF32 for faster training on Ampere GPUs (RTX 30xx, A100, etc.)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"✓ TF32 enabled for faster training\")\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8d67bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal batch size for your GPU...\n",
      "Testing batch_size=1... "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>45.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>52.409400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Works! (Memory used: 2.47 GB)\n",
      "Testing batch_size=2... "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>48.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>54.880700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Works! (Memory used: 2.47 GB)\n",
      "Testing batch_size=4... "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>47.497800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>50.153700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Works! (Memory used: 2.47 GB)\n",
      "Testing batch_size=8... "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>47.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>50.849600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Works! (Memory used: 3.32 GB)\n",
      "Testing batch_size=16... "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>48.690600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>49.441100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Works! (Memory used: 5.15 GB)\n",
      "\n",
      "✓ Optimal batch size: 16\n",
      "  Recommended: batch_size=16, gradient_accumulation=8\n",
      "  Effective batch size: 128\n",
      "\n",
      "Optimal batch size determined: 16\n"
     ]
    }
   ],
   "source": [
    "# Find optimal batch size for your GPU\n",
    "def find_max_batch_size(test_model, test_dataset, max_test=16):\n",
    "    \"\"\"\n",
    "    Binary search to find the maximum batch size that fits in GPU memory.\n",
    "    Returns the largest batch size that doesn't cause OOM errors.\n",
    "    \"\"\"\n",
    "    print(\"Finding optimal batch size for your GPU...\")\n",
    "    max_working_batch = 1\n",
    "    \n",
    "    for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "        if batch_size > max_test:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            print(f\"Testing batch_size={batch_size}...\", end=\" \")\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            test_args = TrainingArguments(\n",
    "                output_dir=\"./temp\",\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                max_steps=2,\n",
    "                logging_steps=1,\n",
    "                gradient_checkpointing=True,\n",
    "                fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "                bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "            )\n",
    "            \n",
    "            test_trainer = Trainer(\n",
    "                model=test_model,\n",
    "                args=test_args,\n",
    "                train_dataset=test_dataset.select(range(min(50, len(test_dataset)))),\n",
    "            )\n",
    "            \n",
    "            test_trainer.train()\n",
    "            print(f\"✓ Works! (Memory used: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB)\")\n",
    "            max_working_batch = batch_size\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"✗ OOM - Maximum batch size is {max_working_batch}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    print(f\"\\n✓ Optimal batch size: {max_working_batch}\")\n",
    "    print(f\"  Recommended: batch_size={max_working_batch}, gradient_accumulation=8\")\n",
    "    print(f\"  Effective batch size: {max_working_batch * 8}\")\n",
    "    return max_working_batch\n",
    "\n",
    "\n",
    "optimal_batch_size = find_max_batch_size(model, tokenized_datasets[\"train\"])\n",
    "print(f\"\\nOptimal batch size determined: {optimal_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b85e4",
   "metadata": {},
   "source": [
    "### 4.2 - Improved Data Preprocessing\n",
    "\n",
    "**Current Issue**: Only using 2% of available data (every 50th sample)\n",
    "\n",
    "**Solution**: Use more or all of the training data for better model learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6968d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Original dataset sizes:\n",
      "  Train: 12,460 samples\n",
      "  Validation: 500 samples\n",
      "  Test: 1,500 samples\n",
      "\n",
      "Current tokenized dataset (filtered every 50th):\n",
      "  Train: 250 samples (2.0%)\n",
      "  Validation: 10 samples (2.0%)\n",
      "  Test: 30 samples (2.0%)\n",
      "\n",
      "Data statistics (sample of 1000):\n",
      "  Dialogue length - Mean: 130.1, Median: 118.0, Std: 67.0\n",
      "  Summary length - Mean: 22.4, Median: 21.0, Std: 10.3\n",
      "  Compression ratio: 5.81x\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze current dataset usage\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Original dataset sizes\n",
    "print(f\"\\nOriginal dataset sizes:\")\n",
    "print(f\"  Train: {len(dataset['train']):,} samples\")\n",
    "print(f\"  Validation: {len(dataset['validation']):,} samples\")\n",
    "print(f\"  Test: {len(dataset['test']):,} samples\")\n",
    "\n",
    "# Current tokenized dataset (with filtering)\n",
    "print(f\"\\nCurrent tokenized dataset (filtered every 50th):\")\n",
    "print(f\"  Train: {len(tokenized_datasets['train']):,} samples ({len(tokenized_datasets['train'])/len(dataset['train'])*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(tokenized_datasets['validation']):,} samples ({len(tokenized_datasets['validation'])/len(dataset['validation'])*100:.1f}%)\")\n",
    "print(f\"  Test: {len(tokenized_datasets['test']):,} samples ({len(tokenized_datasets['test'])/len(dataset['test'])*100:.1f}%)\")\n",
    "\n",
    "# Analyze dialogue and summary lengths\n",
    "dialogue_lengths = [len(ex[\"dialogue\"].split()) for ex in dataset[\"train\"].select(range(min(1000, len(dataset[\"train\"]))))]\n",
    "summary_lengths = [len(ex[\"summary\"].split()) for ex in dataset[\"train\"].select(range(min(1000, len(dataset[\"train\"]))))]\n",
    "\n",
    "print(f\"\\nData statistics (sample of 1000):\")\n",
    "print(f\"  Dialogue length - Mean: {np.mean(dialogue_lengths):.1f}, Median: {np.median(dialogue_lengths):.1f}, Std: {np.std(dialogue_lengths):.1f}\")\n",
    "print(f\"  Summary length - Mean: {np.mean(summary_lengths):.1f}, Median: {np.median(summary_lengths):.1f}, Std: {np.std(summary_lengths):.1f}\")\n",
    "print(f\"  Compression ratio: {np.mean(dialogue_lengths) / np.mean(summary_lengths):.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13ca0927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating improved tokenized datasets...\n",
      "\n",
      "Choose one of the following options:\n",
      "\n",
      "Option 1: USE ALL DATA (100%)\n",
      "  - Best performance, longer training time (~3-8 hours)\n",
      "  - Recommended if you have time for overnight training\n",
      "  Train samples: 12,460\n",
      "\n",
      "Option 2: USE 25% OF DATA\n",
      "  - Good balance of performance and speed (~1-2 hours)\n",
      "  - Recommended for initial improvements\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca1fbf8d59b480e884ccf937ad07372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b19a36d419b40bf93f6412d6f5832d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971c1a24d8554db39d0686b74de4e451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train samples: 3,115\n",
      "\n",
      "Option 3: USE 10% OF DATA\n",
      "  - Faster training (~30-60 minutes)\n",
      "  - Good for testing configurations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d75d48158b745eb93fb9179370ab8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4dea61300341ecabd5d6b795c6b8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57841b1b0bf04a778679ebc97f0687bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train samples: 1,246\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION: Start with Option 2 (25%) to see quick improvements,\n",
      "then train with Option 1 (100%) overnight for best results.\n",
      "================================================================================\n",
      "\n",
      "✓ Using dataset with 3,115 training samples\n"
     ]
    }
   ],
   "source": [
    "# Create improved tokenized datasets with different data usage options\n",
    "\n",
    "print(\"Creating improved tokenized datasets...\")\n",
    "print(\"\\nChoose one of the following options:\\n\")\n",
    "\n",
    "# Option 1: Use ALL data (recommended for best performance)\n",
    "print(\"Option 1: USE ALL DATA (100%)\")\n",
    "print(\"  - Best performance, longer training time (~3-8 hours)\")\n",
    "print(\"  - Recommended if you have time for overnight training\")\n",
    "tokenized_datasets_full = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets_full = tokenized_datasets_full.remove_columns([\"dialogue\", \"summary\", \"topic\", \"id\"])\n",
    "tokenized_datasets_full.set_format(\"torch\")\n",
    "print(f\"  Train samples: {len(tokenized_datasets_full['train']):,}\")\n",
    "\n",
    "# Option 2: Use 25% of data (good balance)\n",
    "print(\"\\nOption 2: USE 25% OF DATA\")\n",
    "print(\"  - Good balance of performance and speed (~1-2 hours)\")\n",
    "print(\"  - Recommended for initial improvements\")\n",
    "tokenized_datasets_25pct = tokenized_datasets_full.filter(lambda example, idx: idx % 4 == 0, with_indices=True)\n",
    "print(f\"  Train samples: {len(tokenized_datasets_25pct['train']):,}\")\n",
    "\n",
    "# Option 3: Use 10% of data (faster training)\n",
    "print(\"\\nOption 3: USE 10% OF DATA\")\n",
    "print(\"  - Faster training (~30-60 minutes)\")\n",
    "print(\"  - Good for testing configurations\")\n",
    "tokenized_datasets_10pct = tokenized_datasets_full.filter(lambda example, idx: idx % 10 == 0, with_indices=True)\n",
    "print(f\"  Train samples: {len(tokenized_datasets_10pct['train']):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATION: Start with Option 2 (25%) to see quick improvements,\")\n",
    "print(\"then train with Option 1 (100%) overnight for best results.\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set the dataset you want to use (change this variable to switch)\n",
    "# Options: tokenized_datasets_full, tokenized_datasets_25pct, tokenized_datasets_10pct\n",
    "improved_tokenized_datasets = tokenized_datasets_25pct  # Change this as needed\n",
    "\n",
    "print(f\"\\n✓ Using dataset with {len(improved_tokenized_datasets['train']):,} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e67c54",
   "metadata": {},
   "source": [
    "### 4.3 - Optimized Training Configuration for Full Fine-tuning\n",
    "\n",
    "**Current Issues**:\n",
    "- `max_steps=1` (essentially no training!)\n",
    "- Only 1 epoch\n",
    "- Small batch size\n",
    "\n",
    "**Solutions**: Proper training parameters with GPU optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e811c16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IMPROVED TRAINING CONFIGURATION\n",
      "================================================================================\n",
      "Epochs: 3\n",
      "Max steps: -1 (-1 = use epochs)\n",
      "Learning rate: 2e-05\n",
      "Per-device batch size: 2\n",
      "Gradient accumulation: 8\n",
      "Effective batch size: 16\n",
      "Precision: BF16\n",
      "Total training steps: ~582\n",
      "================================================================================\n",
      "\n",
      "✓ Improved trainer created!\n",
      "\n",
      "To train: improved_trainer.train()\n",
      "Expected training time: ~1-3 hours (depending on GPU and data size)\n",
      "\n",
      "✓ Improved trainer created!\n",
      "\n",
      "To train: improved_trainer.train()\n",
      "Expected training time: ~1-3 hours (depending on GPU and data size)\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED Training Configuration for Full Fine-tuning\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Clear GPU memory before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "improved_training_args = TrainingArguments(\n",
    "    output_dir=\"./results-improved\",\n",
    "    \n",
    "    # LEARNING RATE & OPTIMIZATION\n",
    "    learning_rate=2e-5,              # Standard for T5 fine-tuning\n",
    "    lr_scheduler_type=\"cosine\",      # Cosine annealing for better convergence\n",
    "    warmup_ratio=0.1,                # Warmup for 10% of training\n",
    "    weight_decay=0.01,               # L2 regularization\n",
    "    max_grad_norm=1.0,               # Gradient clipping\n",
    "    \n",
    "    # TRAINING DURATION - CRITICAL FIX!\n",
    "    num_train_epochs=3,              # 3 epochs instead of 1\n",
    "    max_steps=-1,                    # REMOVE THE LIMIT! (was max_steps=1)\n",
    "    \n",
    "    # BATCH SIZE & ACCUMULATION\n",
    "    per_device_train_batch_size=2,   # Adjust based on your GPU (2-8)\n",
    "    per_device_eval_batch_size=4,    # Can be higher for evaluation\n",
    "    gradient_accumulation_steps=8,   # Effective batch size = 2 * 8 = 16\n",
    "    \n",
    "    # MEMORY OPTIMIZATION\n",
    "    gradient_checkpointing=True,     # Save memory at cost of speed\n",
    "    fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    \n",
    "    # LOGGING & EVALUATION\n",
    "    logging_dir=\"./results-improved/logs\",\n",
    "    logging_steps=200,                # Log every 200 steps\n",
    "    eval_strategy=\"steps\",           # Evaluate during training\n",
    "    eval_steps=200,                  # Evaluate every 200 steps\n",
    "    save_strategy=\"steps\",           # Save checkpoints\n",
    "    save_steps=200,                  # Save every 200 steps\n",
    "    save_total_limit=3,              # Keep only 3 best checkpoints\n",
    "    load_best_model_at_end=True,     # Load best checkpoint at end\n",
    "    metric_for_best_model=\"eval_loss\", # Use eval loss to determine best model\n",
    "    \n",
    "    # PERFORMANCE\n",
    "    report_to=\"none\",                # Disable external logging (tensorboard requires installation)\n",
    "    dataloader_num_workers=2,        # Parallel data loading\n",
    "    \n",
    "    # REPRODUCIBILITY\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"IMPROVED TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Epochs: {improved_training_args.num_train_epochs}\")\n",
    "print(f\"Max steps: {improved_training_args.max_steps} (-1 = use epochs)\")\n",
    "print(f\"Learning rate: {improved_training_args.learning_rate}\")\n",
    "print(f\"Per-device batch size: {improved_training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {improved_training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {improved_training_args.per_device_train_batch_size * improved_training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Precision: {'BF16' if improved_training_args.bf16 else 'FP16' if improved_training_args.fp16 else 'FP32'}\")\n",
    "print(f\"Total training steps: ~{len(improved_tokenized_datasets['train']) // (improved_training_args.per_device_train_batch_size * improved_training_args.gradient_accumulation_steps) * improved_training_args.num_train_epochs}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create trainer with early stopping\n",
    "improved_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=improved_training_args,\n",
    "    train_dataset=improved_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=improved_tokenized_datasets[\"validation\"],\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Stop if no improvement for 3 evals\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Improved trainer created!\")\n",
    "print(\"\\nTo train: improved_trainer.train()\")\n",
    "print(f\"Expected training time: ~1-3 hours (depending on GPU and data size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95bfcdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting improved training...\n",
      "GPU Device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Total Memory: 6.00 GB\n",
      "Allocated: 1.47 GB\n",
      "Reserved: 1.49 GB\n",
      "Free: 4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [585/585 34:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>39.141400</td>\n",
       "      <td>27.624212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>15.002200</td>\n",
       "      <td>6.134187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Optional: Train the improved model\n",
    "\n",
    "print(\"Starting improved training...\")\n",
    "print_gpu_memory()\n",
    "improved_trainer.train()\n",
    "print(\"\\n✓ Training complete!\")\n",
    "\n",
    "# print(\"Ready to train! Uncomment the lines above to start training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetune-conversation-summary-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
